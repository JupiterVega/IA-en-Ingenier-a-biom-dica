{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Caso práctico U-NET con uso de LUNA dataset\nEl LUng Nodule Analysis 2016 (LUNA16) se centra en la detección de nódulos pulmonares a partir de tomografías computarizadas (CT) de dosis baja, con el objetivo de mejorar el diagnóstico temprano de cáncer de pulmón, la principal causa de muerte relacionada con cáncer en el mundo.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Configuración e instalación de bibliotecas\nSe realiza la configuración e instalación de las bibliotecas necesarias para llevar a cabo la implementación de la arquitectura U-Net en el análisis de imágenes del dataset LUNA16.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Instalación de dependencias\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%pip install tensorflow\n%pip install pillow\n%pip install scikit-image\n%pip install matplotlib\n%pip install scikit-learn\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Importación de bibliotecas\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import tensorflow as tf  # Importa TensorFlow, usado para construir y entrenar la U-Net\nimport os  # Para operaciones del sistema de archivos, como listar directorios\nimport random  # Para la generación de números aleatorios (e.g., para divisiones aleatorias de datos)\nimport numpy as np  # Biblioteca de cálculo numérico eficiente\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Importación de módulos específicos\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from tqdm import tqdm  # Barra de progreso para bucles largos\nfrom PIL import Image  # Manejo de imágenes\nfrom skimage.io import imread, imshow  # Carga y visualización de imágenes\nfrom skimage.transform import resize  # Redimensionado de imágenes\nimport matplotlib.pyplot as plt  # Visualización de datos\nfrom sklearn.model_selection import train_test_split  # División del dataset en entrenamiento y prueba\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Importación de Keras y TensorFlow para modelos\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from keras import layers  # Capas utilizadas en la red neuronal\nimport keras  # Biblioteca de alto nivel para crear redes neuronales (encima de TensorFlow)\nfrom IPython.display import Image, display  # Muestra imágenes en un notebook Jupyter\nfrom keras.utils import load_img  # Carga de imágenes (similar a PIL)\nfrom keras import utils  # Utilidades de Keras para el manejo de datos\nfrom PIL import ImageOps  # Para aplicar operaciones de imagen (como reflejos, rotaciones)\nfrom tensorflow import data as tf_data  # API de manejo de datos en TensorFlow\nfrom tensorflow import image as tf_image  # API de operaciones específicas de imágenes en TensorFlow\nfrom tensorflow import io as tf_io  # API para operaciones de entrada/salida\nfrom tensorflow.keras import Model, models  # Clases base para la creación de modelos en Keras\nfrom tensorflow.keras.applications import ResNet50  # Modelo preentrenado que se puede usar como base\nfrom tensorflow.keras.optimizers import Adam  # Optimizador específico para el entrenamiento del modelo\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Preprocesamiento de datos\nComienza con la extracción de archivos comprimidos (en este caso, archivos .zip) que contienen las imágenes para el entrenamiento y la prueba.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Extracción inicial de archivos\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from zipfile import ZipFile\n\n# Extraer archivos del archivo ZIP a las carpetas correspondientes\nwith ZipFile(\"C:\\\\Users\\\\PC\\\\Downloads\\\\data-science-bowl-2018.zip\", \"r\") as zip_ref:\n    zip_ref.extractall(\"./stage1_train\")  # Extraer a la carpeta de entrenamiento\n\n# Extraer nuevamente el ZIP para separar datos de prueba, si es necesario\nwith ZipFile(\"C:\\\\Users\\\\PC\\\\Downloads\\\\data-science-bowl-2018.zip\", \"r\") as zip_ref:\n    zip_ref.extractall(\"./stage1_test\")  # Extraer a la carpeta de prueba\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Función para extraer archivos ZIP de un directorio\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import os\nfrom zipfile import ZipFile\n\n# Funcion para extraer archivos ZIP\ndef extract_zip_files(directory):\n    for filename in os.listdir(directory):\n        if filename.endswith('.zip'):\n            zip_path = os.path.join(directory, filename)\n            with ZipFile(zip_path, 'r') as zip_ref:\n                zip_ref.extractall(directory)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Uso de la función de extracción\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#Extraer archivos ZIP de stage1_train\nextract_zip_files('./stage1_train')\n\n#Extraer archivos ZIP de stage1_test\nextract_zip_files('./stage1_test')\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Listado de directorios\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(os.listdir('./stage1_train'))\nprint(os.listdir('./stage1_test'))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Preprocesamiento de imágenes\nSe cubre especialmente el ajuste de tamaño, la normalización y la preparación de las máscaras para el entrenamiento del modelo U-Net.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Dimensiones y configuraciones iniciales\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "IMG_WIDTH = 128\nIMG_HEIGHT = 128\nIMG_CHANNELS = 3\n\nTRAIN_PATH = 'stage1_train/'\nTEST_PATH = 'stage1_test/'\n\ntrain_ids = [d for d in os.listdir(TRAIN_PATH) if os.path.isdir(os.path.join(TRAIN_PATH, d))]\ntest_ids = [d for d in os.listdir(TEST_PATH) if os.path.isdir(os.path.join(TEST_PATH, d))]\n\nx = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\ny = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=bool)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Verificación del contenido de las carpetas\n",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        }
      }
    },
    {
      "cell_type": "code",
      "source": "import os\n\n# Verificar el contenido de las carpetas de entrenamiento y prueba\nprint(\"Contenido de la carpeta de entrenamiento:\")\nprint(os.listdir('./stage1_train'))\n\nprint(\"Contenido de la carpeta de prueba:\")\nprint(os.listdir('./stage1_test'))\n",
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Redimensionamiento y preprocesamiento de imágenes de entrenamiento\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import os\nimport numpy as np\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom tqdm import tqdm\n\nprint('Resizing training images and masks')\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n    path = TRAIN_PATH + id_\n    img_path = os.path.join(path, 'images', f'{id_}.png')\n    img = imread(img_path)\n\n    # Verificar si la imagen es 2D (escala de grises) y convertir a 3D\n    if img.ndim == 2:  # Escala de grises\n        img = np.stack((img,)*3, axis=-1)  # Convertir a RGB duplicando el canal\n    elif img.shape[2] > 3:  # Si hay más de 3 canales, tomar solo los primeros 3\n        img = img[:, :, :3]  # Eliminar el canal alfa si existe\n    elif img.shape[2] < 3:  # Si hay menos de 3 canales, puedes duplicar\n        img = np.stack((img,)*3, axis=-1)\n\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    x[n] = img  # Almacena la imagen redimensionada en x\n    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=bool)\n\n    # Recorrer todas las subcarpetas en el directorio de máscaras\n    masks_path = os.path.join(path, 'masks')\n    for dirpath, dirnames, filenames in os.walk(masks_path):\n        for mask_file in filenames:\n            mask_ = imread(os.path.join(dirpath, mask_file))\n            mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True), axis=-1)\n            mask = np.maximum(mask, mask_)\n\n    y[n] = mask  # Almacena la máscara en y\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Visualizacion de la estructura de las carpetas\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import os\n\n# Listar contenido de la carpeta de entrenamiento\nprint(\"Contenido de la carpeta de entrenamiento:\")\nfor root, dirs, files in os.walk('stage1_train'):\n    print(f\"Directorio: {root}\")\n    for file in files:\n        print(f\"  Archivo: {file}\")\n    for dir in dirs:\n        print(f\"  Subcarpeta: {dir}\")\n\n# Listar contenido de la carpeta de prueba\nprint(\"Contenido de la carpeta de prueba:\")\nfor root, dirs, files in os.walk('stage1_test'):\n    print(f\"Directorio: {root}\")\n    for file in files:\n        print(f\"  Archivo: {file}\")\n    for dir in dirs:\n        print(f\"  Subcarpeta: {dir}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Visualización de IDs de entrenamiento y prueba\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(train_ids)\nprint(test_ids)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### División del dataset en entrenamiento y prueba\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Verificación de las dimensiones del conjunto de datos\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "x_train[0].shape\nx_train.shape\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Visualización aleatoria de una imágen de entrenamiento y su máscara correspondiente\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "image_x = random.randint(0, len(x_train))\nplt.axis(\"off\")\nimshow(x_train[image_x])\nplt.show()\n\nplt.axis(\"off\")\nimshow(np.squeeze(y_train[image_x]))\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Preparación de imágenes de prueba\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "test_images = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nsizes_test = []\nprint('Resizing test images')\n\nfor n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n    path = os.path.join(TEST_PATH, id_)\n    img_path = os.path.join(path, 'images', f'{id_}.png')\n    img = imread(img_path)\n\n    # Verificar las dimensiones de la imagen\n    if img.ndim == 2:  # Escala de grises\n        img = np.stack((img,)*3, axis=-1)  # Convertir a RGB duplicando el canal\n    elif img.shape[2] > 3:  # Si hay más de 3 canales, tomar solo los primeros 3\n        img = img[:, :, :3]  # Eliminar el canal alfa si existe\n    elif img.shape[2] < 3:  # Si hay menos de 3 canales, puedes duplicar\n        img = np.stack((img,)*3, axis=-1)\n\n    sizes_test.append([img.shape[0], img.shape[1]])\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    test_images[n] = img\n\nprint('Done!')\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Definición de parámetros de entrada para la U-Net\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "input_shape = (128, 128, 3)\nnum_classes = 1\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Construcción de la U-Net\nEsta sección contiene la implementación de la arquitectura U-Net para la segmentación de imágenes, además de la creación de un modelo ResNet. También hay partes dedicadas a la visualización de los resultados y predicciones con diferentes arquitecturas.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Definición del modelo U-Net\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n\n# Contraction path (Encoder)\nc1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\nc1 = tf.keras.layers.Dropout(0.1)(c1)\nc1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\nb1 = tf.keras.layers.BatchNormalization()(c1)\nr1 = tf.keras.layers.ReLU()(b1)\np1 = tf.keras.layers.MaxPooling2D((2, 2))(r1)\n\nc2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\nc2 = tf.keras.layers.Dropout(0.1)(c2)\nc2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\nb2 = tf.keras.layers.BatchNormalization()(c2)\nr2 = tf.keras.layers.ReLU()(b2)\np2 = tf.keras.layers.MaxPooling2D((2, 2))(r2)\n\nc3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\nc3 = tf.keras.layers.Dropout(0.2)(c3)\nc3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\nb3 = tf.keras.layers.BatchNormalization()(c3)\nr3 = tf.keras.layers.ReLU()(b3)\np3 = tf.keras.layers.MaxPooling2D((2, 2))(r3)\n\nc4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\nc4 = tf.keras.layers.Dropout(0.2)(c4)\nc4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\nb4 = tf.keras.layers.BatchNormalization()(c4)\nr4 = tf.keras.layers.ReLU()(b4)\np4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(r4)\n\nc5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\nb5 = tf.keras.layers.BatchNormalization()(c5)\nr5 = tf.keras.layers.ReLU()(b5)\nc5 = tf.keras.layers.Dropout(0.3)(r5)\nc5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Decodificar (Expansive Path)\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Expansive path (Decoder)\nu6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\nu6 = tf.keras.layers.concatenate([u6, c4])\nu6 = tf.keras.layers.BatchNormalization()(u6)\nu6 = tf.keras.layers.ReLU()(u6)\n\nu7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(u6)\nu7 = tf.keras.layers.concatenate([u7, c3])\nu7 = tf.keras.layers.BatchNormalization()(u7)\nu7 = tf.keras.layers.ReLU()(u7)\n\nu8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(u7)\nu8 = tf.keras.layers.concatenate([u8, c2])\nu8 = tf.keras.layers.BatchNormalization()(u8)\nu8 = tf.keras.layers.ReLU()(u8)\n\nu9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(u8)\nu9 = tf.keras.layers.concatenate([u9, c1], axis=3)\nu9 = tf.keras.layers.BatchNormalization()(u9)\nu9 = tf.keras.layers.ReLU()(u9)\n\noutputs = tf.keras.layers.Conv2D(num_classes, (1, 1), activation='sigmoid')(u9)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Compilación y entrenamiento del modelo U-Net\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model_unet = tf.keras.Model(inputs=[inputs], outputs=[outputs])\nmodel_unet.compile(optimizer=Adam(learning_rate=0.005), loss='binary_crossentropy', metrics=['accuracy'])\nmodel_unet.summary()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Callbacks y Entrenamiento \n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "callbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n    tf.keras.callbacks.TensorBoard(log_dir='logs')\n]\n\nmodel_unet.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=16, epochs=25, callbacks=callbacks)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Visualización de resultados de entrenamiento\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "accuracy = model_unet.history.history['accuracy']\nval_accuracy = model_unet.history.history['val_accuracy']\n\nplt.figure()\nplt.plot(accuracy, 'r', label='Training accuracy')\nplt.plot(val_accuracy, 'bo', label='Validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Loss Value')\nplt.ylim([0, 1])\nplt.legend()\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Implementación del modelo ResNet\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model_resnet_tf = tf.keras.Sequential([\n    ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(num_classes, activation='softmax')\n])\n\nmodel_resnet_tf.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Visualización y predicción\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def display(display_list):\n    plt.figure(figsize=(15, 5))\n    title = ['Input Image', 'Ground Truth', 'Predicted Mask']\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i + 1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n        plt.axis('off')\n    plt.show()\n\n# Predicción de máscaras usando diferentes modelos y visualización\ndisplay([sample_image, sample_mask, predicted_mask_unet])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Preparación de la muestra para la predicción\nRealización de predicciones con diferentes modelos (U-Net y ResNet), así como en la visualización de los resultados de segmentación.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Selección aleatoria de una imágen de prueba\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "i = random.randint(0, len(x_test))\nsample_image = x_test[i]\nsample_mask = y_test[i]\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Predicción usando diferentes modelos\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Lista de modelos para predecir\nmodels = [model_unet, model_resnet, model_resnet_tf]\n\n# Diccionario para almacenar predicciones\npredictions = {}\n\n# Iterar a través de cada modelo y hacer predicciones.\nfor model in models:\n    model_name = model.__class__.__name__  # Obtener el nombre de clase del modelo.\n    try:\n        predictions[model_name] = model.predict(sample_image[tf.newaxis, ...])[0]\n    except Exception as e:\n        predictions[model_name] = f\"Error: {str(e)}\"  # Almacenar el mensaje de error\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Uso de modelos con nombres personalizados\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Lista de modelos para predecir con sus nombres personalizados\nmodel_info = {\n    \"U-Net\": model_unet,\n    \"ResNet\": model_resnet,\n    \"ResNetTF\": model_resnet_tf,\n}\n\n# Diccionario para almacenar predicciones\npredictions = {}\n\n# Iterar a través de cada modelo y hacer predicciones.\nfor model_name, model in model_info.items():\n    try:\n        predictions[model_name] = model.predict(sample_image[tf.newaxis, ...])[0]\n    except Exception as e:\n        predictions[model_name] = f\"Error: {str(e)}\"  # Almacenar el mensaje de error\n\n# Imprime las claves del diccionario de predicciones\nprint(predictions.keys())\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Instalación de OpenCV para visualización adicional\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%pip install opencv-python\n\nimport cv2\n\n# Cargue la imagen (asegúrese de proporcionar una ruta válida)\nsample_image = cv2.imread('path_to_image.jpg')\n\n# Compruebe si la imagen está cargada correctamente\nif sample_image is not None:\n    sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)  # Convertir a RGB si es necesario\n    print(\"Sample image shape:\", sample_image.shape)\nelse:\n    print(\"Error loading image!\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Comprobación de formatos de las imágenes y visualización \n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Check the shapes\nprint(\"Sample image shape:\", sample_image.shape)\nprint(\"Sample mask shape:\", sample_mask.shape)\nprint(\"Predicted mask U-Net shape:\", predicted_mask_unet.shape)\n\n# Asegúrese de que predicted_mask_unet tenga la forma correcta\nif predicted_mask_unet.ndim == 2:  # Si es 2D (alto, ancho)\n    predicted_mask_unet = np.expand_dims(predicted_mask_unet, axis=-1)  # Agregar una dimensión de canal\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Visualización de resultados\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def display(display_list):\n    title = ['Input Image', 'Ground Truth', 'Predicted Mask']\n    plt.figure(figsize=(15, 5))\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i + 1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n        plt.axis('off')\n    plt.show()\n\n# Llame a la función de visualización\ndisplay([sample_image, sample_mask, predicted_mask_unet])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Visualización para diferentes modelos\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "display([sample_image, sample_mask, predicted_mask_resnet])\ndisplay([sample_image, sample_mask, predicted_mask_resnet_tf])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}